{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import os\n",
    "\n",
    "directory = 'C:\\\\Users\\\\chris\\\\tenancy_tribunal\\\\PDFs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = []\n",
    "\n",
    "def visitor_body(text, cm, tm, font_dict, font_size):\n",
    "    y = cm[5]\n",
    "\n",
    "    if y > 100 and y < 800:\n",
    "        parts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tribunal orders\n",
    "tribunal_orders = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    reader = PdfReader(os.path.join(directory,file))\n",
    "\n",
    "    for page in reader.pages[:-2]:\n",
    "        page.extract_text(visitor_text=visitor_body)\n",
    "    \n",
    "    text_body = \"\".join(parts)\n",
    "\n",
    "    tribunal_orders.append(text_body)\n",
    "\n",
    "    parts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet') \n",
    "nltk.download('punkt')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Apply stemming to a set of documents\n",
    "def apply_stemming(documents):\n",
    "    result = []\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for document in documents:\n",
    "        tokens = nltk.word_tokenize(document)\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        stemmed_document = ' '.join(stemmed_tokens)\n",
    "\n",
    "        result.append(stemmed_document)\n",
    "\n",
    "    return(result)\n",
    "\n",
    "\n",
    "# Apply lemmtisation to a set of document\n",
    "def apply_lemmatisation(documents):\n",
    "    result = []\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "    for document in documents:\n",
    "        tokens = nltk.word_tokenize(document)\n",
    "        lemmatised_tokens = [lemmatiser.lemmatize(token) for token in tokens]\n",
    "        lemmatised_document =  ' '.join(lemmatised_tokens)\n",
    "\n",
    "        result.append(lemmatised_document)\n",
    "    \n",
    "    return(result)\n",
    "\n",
    "\n",
    "# Apply entity filtering to a single document\n",
    "def filter_entities(document, entities_to_filter):\n",
    "\n",
    "    doc = nlp(document)\n",
    "    cleaned_text = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.ent_type_ not in entities_to_filter:\n",
    "            cleaned_text.append(token.text)\n",
    "        \n",
    "    filtered_document =  ' '.join(cleaned_text)\n",
    "\n",
    "    \n",
    "    return (filtered_document)\n",
    "\n",
    "# Prepare data for doccano\n",
    "def write_jsonl(file_path, documents):\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        for doc in documents:\n",
    "            document_dict = {\"text\": doc}  # Each string is treated as the 'text' field in a JSON object\n",
    "            file.write(json.dumps(document_dict) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-process text for tfidf\n",
    "def preprocess_tfidf(documents):\n",
    "\n",
    "    result = []\n",
    "    entities_to_filter = ['PERSON', 'ORG', 'CARDINAL', 'MONEY', 'DATE', 'FAC', 'GPE', 'LOC']\n",
    "\n",
    "    for document in documents:\n",
    "\n",
    "        #Filter out entities\n",
    "        document = filter_entities(document, entities_to_filter=entities_to_filter)\n",
    "\n",
    "        # Push to lower text\n",
    "        document = document.lower()\n",
    "\n",
    "        # Remove numbers\n",
    "        document = re.sub(r'\\d+', '', document)\n",
    "\n",
    "        # Remove names\n",
    "        document = re.sub(r'\\b[A-Z][a-z]+\\b', '', document)\n",
    "\n",
    "        # Remove punctuation and special characters\n",
    "        document = re.sub(r'[^a-zA-Z\\s]', '', document)\n",
    "\n",
    "        result.append(document)\n",
    "\n",
    "    #result = apply_stemming(result)\n",
    "    result = apply_lemmatisation(result)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pre-processed datasets for specific tasks\n",
    "tribunal_orders_tfidf = preprocess_tfidf(tribunal_orders)\n",
    "tribunal_orders_ner = tribunal_orders\n",
    "tribunal_orders_cat = tribunal_orders\n",
    "tribunal_orders_sum = tribunal_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Processed Tribunal Order\n",
    "\n",
    "Load, process and extract feeatures from tribunal orders. To do this we will compute the below for each document:\n",
    "- Key terms (via TF-IDF)\n",
    "- Named entities (via spaCy)\n",
    "- Classify documents (spaCy, need training data for this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "documents = tribunal_orders_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key terms for Document 1: ['cat', 'door', 'pane', 'glass', 'vacate']\n",
      "Key terms for Document 2: ['behind', 'property', 'premise', 'left', 'helped']\n",
      "Key terms for Document 3: ['damage', 'carpet', 'crack', 'vanity', 'ha']\n",
      "Key terms for Document 4: ['rehearing', 'wa', 'application', 'allegation', 'matter']\n",
      "Key terms for Document 5: ['basement', 'notice', 'quiet', 'retaliatory', 'wa']\n",
      "Key terms for Document 6: ['damage', 'rent', 'stay', 'premise', 'window']\n",
      "Key terms for Document 7: ['damage', 'doctor', 'insurance', 'see', 'rubbish']\n",
      "Key terms for Document 8: ['bond', 'party', 'end', 'rent', 'filing']\n",
      "Key terms for Document 9: ['bond', 'party', 'end', 'rent', 'filing']\n",
      "Key terms for Document 10: ['party', 'irrigation', 'system', 'damage', 'harassed']\n",
      "Key terms for Document 11: ['arrears', 'rate', 'nicola', 'water', 'bond']\n",
      "Key terms for Document 12: ['owner', 'bond', 'wa', 'insulation', 'standard']\n",
      "Key terms for Document 13: ['damage', 'rent', 'owned', 'property', 'arrears']\n",
      "Key terms for Document 14: ['damage', 'rent', 'owned', 'property', 'arrears']\n",
      "Key terms for Document 15: ['breach', 'papaiti', 'terminate', 'change', 'refuse']\n",
      "Key terms for Document 16: ['anaru', 'rent', 'name', 'call', 'arrears']\n",
      "Key terms for Document 17: ['threatened', 'belonging', 'prohibited', 'flat', 'telephone']\n",
      "Key terms for Document 18: ['damage', 'repair', 'carpet', 'door', 'property']\n",
      "Key terms for Document 19: ['family', 'owner', 'member', 'trustee', 'premise']\n",
      "Key terms for Document 20: ['methamphetamine', 'breach', 'exemplary', 'consumed', 'unlawful']\n",
      "Key terms for Document 21: ['dog', 'pet', 'keep', 'premise', 'appendix']\n",
      "Key terms for Document 22: ['loss', 'dampness', 'enjoyment', 'leaking', 'wa']\n",
      "Key terms for Document 23: ['termination', 'kishore', 'manawatu', 'broker', 'arrears']\n",
      "Key terms for Document 24: ['flooding', 'drainage', 'standard', 'effective', 'certificate']\n",
      "Key terms for Document 25: ['incurred', 'lease', 'cost', 'surrender', 'break']\n",
      "Key terms for Document 26: ['prospective', 'viewing', 'premise', 'declaration', 'consent']\n",
      "Key terms for Document 27: ['damage', 'harcourts', 'management', 'see', 'replace']\n",
      "Key terms for Document 28: ['damage', 'replace', 'light', 'garage', 'extensive']\n",
      "Key terms for Document 29: ['damage', 'cleaning', 'dishwasher', 'work', 'rent']\n",
      "Key terms for Document 30: ['damage', 'rent', 'gate', 'see', 'expense']\n",
      "Key terms for Document 31: ['damage', 'reconnection', 'rate', 'rent', 'power']\n",
      "Key terms for Document 32: ['damage', 'invoice', 'dated', 'quantity', 'carpet']\n",
      "Key terms for Document 33: ['agent', 'damage', 'electricity', 'drain', 'toilet']\n",
      "Key terms for Document 34: ['mr', 'hob', 'power', 'choudry', 'key']\n",
      "Key terms for Document 35: ['rate', 'water', 'rent', 'arrears', 'bond']\n",
      "Key terms for Document 36: ['garden', 'floor', 'surface', 'damage', 'condition']\n",
      "Key terms for Document 37: ['owner', 'caravan', 'sleepout', 'agent', 'application']\n",
      "Key terms for Document 38: ['agent', 'flat', 'unit', 'arrears', 'bond']\n",
      "Key terms for Document 39: ['onehunga', 'breach', 'rent', 'ha', 'water']\n",
      "Key terms for Document 40: ['remedy', 'breach', 'arrears', 'notice', 'rent']\n",
      "Key terms for Document 41: ['counterclaim', 'bond', 'filed', 'kilian', 'ernest']\n",
      "Key terms for Document 42: ['inspection', 'access', 'notice', 'entry', 'visit']\n",
      "Key terms for Document 43: ['possession', 'returning', 'wa', 'reasonably', 'returned']\n",
      "Key terms for Document 44: ['gardiner', 'trust', 'family', 'damage', 'premise']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "stop_words.append('tenant')\n",
    "stop_words.append('landlord')\n",
    "stop_words.append('tenancy')\n",
    "stop_words.append('suppressed')\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (terms)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a dictionary to store key terms for each document\n",
    "key_terms_per_document = defaultdict(list)\n",
    "\n",
    "# Extract top N key terms per document\n",
    "num_key_terms = 5  # Number of key terms to extract per document\n",
    "\n",
    "for i, document in enumerate(documents):\n",
    "    feature_index = tfidf_matrix[i, :].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
    "\n",
    "    # Sort terms by their TF-IDF score\n",
    "    sorted_items = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Extract top N key terms for each document\n",
    "    for term_index, score in sorted_items[:num_key_terms]:\n",
    "        key_terms_per_document[i].append(feature_names[term_index])\n",
    "\n",
    "# Print key terms for each document\n",
    "#for doc_index, terms in key_terms_per_document.items():\n",
    "#    print(f\"Key terms for Document {doc_index + 1}: {terms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: ['cat', 'door', 'pane', 'glass', 'vacate'],\n",
       "             1: ['behind', 'property', 'premise', 'left', 'helped'],\n",
       "             2: ['damage', 'carpet', 'crack', 'vanity', 'ha'],\n",
       "             3: ['rehearing', 'wa', 'application', 'allegation', 'matter'],\n",
       "             4: ['basement', 'notice', 'quiet', 'retaliatory', 'wa'],\n",
       "             5: ['damage', 'rent', 'stay', 'premise', 'window'],\n",
       "             6: ['damage', 'doctor', 'insurance', 'see', 'rubbish'],\n",
       "             7: ['bond', 'party', 'end', 'rent', 'filing'],\n",
       "             8: ['bond', 'party', 'end', 'rent', 'filing'],\n",
       "             9: ['party', 'irrigation', 'system', 'damage', 'harassed'],\n",
       "             10: ['arrears', 'rate', 'nicola', 'water', 'bond'],\n",
       "             11: ['owner', 'bond', 'wa', 'insulation', 'standard'],\n",
       "             12: ['damage', 'rent', 'owned', 'property', 'arrears'],\n",
       "             13: ['damage', 'rent', 'owned', 'property', 'arrears'],\n",
       "             14: ['breach', 'papaiti', 'terminate', 'change', 'refuse'],\n",
       "             15: ['anaru', 'rent', 'name', 'call', 'arrears'],\n",
       "             16: ['threatened',\n",
       "              'belonging',\n",
       "              'prohibited',\n",
       "              'flat',\n",
       "              'telephone'],\n",
       "             17: ['damage', 'repair', 'carpet', 'door', 'property'],\n",
       "             18: ['family', 'owner', 'member', 'trustee', 'premise'],\n",
       "             19: ['methamphetamine',\n",
       "              'breach',\n",
       "              'exemplary',\n",
       "              'consumed',\n",
       "              'unlawful'],\n",
       "             20: ['dog', 'pet', 'keep', 'premise', 'appendix'],\n",
       "             21: ['loss', 'dampness', 'enjoyment', 'leaking', 'wa'],\n",
       "             22: ['termination', 'kishore', 'manawatu', 'broker', 'arrears'],\n",
       "             23: ['flooding',\n",
       "              'drainage',\n",
       "              'standard',\n",
       "              'effective',\n",
       "              'certificate'],\n",
       "             24: ['incurred', 'lease', 'cost', 'surrender', 'break'],\n",
       "             25: ['prospective',\n",
       "              'viewing',\n",
       "              'premise',\n",
       "              'declaration',\n",
       "              'consent'],\n",
       "             26: ['damage', 'harcourts', 'management', 'see', 'replace'],\n",
       "             27: ['damage', 'replace', 'light', 'garage', 'extensive'],\n",
       "             28: ['damage', 'cleaning', 'dishwasher', 'work', 'rent'],\n",
       "             29: ['damage', 'rent', 'gate', 'see', 'expense'],\n",
       "             30: ['damage', 'reconnection', 'rate', 'rent', 'power'],\n",
       "             31: ['damage', 'invoice', 'dated', 'quantity', 'carpet'],\n",
       "             32: ['agent', 'damage', 'electricity', 'drain', 'toilet'],\n",
       "             33: ['mr', 'hob', 'power', 'choudry', 'key'],\n",
       "             34: ['rate', 'water', 'rent', 'arrears', 'bond'],\n",
       "             35: ['garden', 'floor', 'surface', 'damage', 'condition'],\n",
       "             36: ['owner', 'caravan', 'sleepout', 'agent', 'application'],\n",
       "             37: ['agent', 'flat', 'unit', 'arrears', 'bond'],\n",
       "             38: ['onehunga', 'breach', 'rent', 'ha', 'water'],\n",
       "             39: ['remedy', 'breach', 'arrears', 'notice', 'rent'],\n",
       "             40: ['counterclaim', 'bond', 'filed', 'kilian', 'ernest'],\n",
       "             41: ['inspection', 'access', 'notice', 'entry', 'visit'],\n",
       "             42: ['possession', 'returning', 'wa', 'reasonably', 'returned'],\n",
       "             43: ['gardiner', 'trust', 'family', 'damage', 'premise']})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save TF-IDF key terms for each tribunal order\n",
    "tribunal_tfidf = key_terms_per_document\n",
    "tribunal_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get the documents \n",
    "documents = tribunal_orders_ner\n",
    "\n",
    "# Process the text using SpaCy\n",
    "result = []\n",
    "\n",
    "for document in documents:\n",
    "    doc = nlp(document)\n",
    "    result.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save named entities to processed tribunal order\n",
    "tribunal_named_entities = defaultdict(list)\n",
    "\n",
    "for idx, doc in enumerate(result):\n",
    "    tribunal_named_entities[idx] = doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the saved model\n",
    "path_to_saved_model = 'document_classification\\\\text_cat_model'\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(path_to_saved_model)\n",
    "\n",
    "# Now, you can use the loaded model for various tasks\n",
    "text = \"The landlord entered the property.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "result = []\n",
    "\n",
    "for order in tribunal_orders_cat:\n",
    "    doc = nlp(order)\n",
    "    result.append(doc)\n",
    "\n",
    "# Save named entities to processed tribunal order\n",
    "tribunal_categories = defaultdict(list)\n",
    "\n",
    "for idx, doc in enumerate(result):\n",
    "    tribunal_categories[idx] = doc.cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load pre-trained BART model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for order in tribunal_orders_sum:\n",
    "\n",
    "    # Sample text for summarization\n",
    "    input_text = order\n",
    "\n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(input_ids, max_length=500, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    result.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save named entities to processed tribunal order\n",
    "tribunal_summaries = defaultdict(list)\n",
    "\n",
    "for idx, sum in enumerate(result):\n",
    "    tribunal_summaries[idx] = sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Chris\\tenancy_tribunal\\end_to_end.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chris/tenancy_tribunal/end_to_end.ipynb#Y106sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m input_sequences \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mquestion: \u001b[39m\u001b[39m{\u001b[39;00mq\u001b[39m}\u001b[39;00m\u001b[39m context: \u001b[39m\u001b[39m{\u001b[39;00mc\u001b[39m}\u001b[39;00m\u001b[39m answer: \u001b[39m\u001b[39m{\u001b[39;00ma\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m q, c, a \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(questions, context, answers)]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chris/tenancy_tribunal/end_to_end.ipynb#Y106sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Tokenization\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Chris/tenancy_tribunal/end_to_end.ipynb#Y106sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m T5Tokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mt5-small\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chris/tenancy_tribunal/end_to_end.ipynb#Y106sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m tokenized_input \u001b[39m=\u001b[39m tokenizer(input_sequences, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chris/tenancy_tribunal/end_to_end.ipynb#Y106sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Labels (target) - treat it as a text-to-text problem\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\tenancy_tribunal\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1070\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_from_config\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1069\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1070\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\tenancy_tribunal\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1058\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1056\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n\u001b[0;32m   1057\u001b[0m \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m-> 1058\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "questions = [\"What is the capital of France?\", \"Who invented the internet?\"]\n",
    "answers = [\"The capital of France is Paris.\", \"The internet was invented by Tim Berners-Lee.\"]\n",
    "context = [\"Paris is known for its beautiful architecture and rich history.\", \"Tim Berners-Lee, a computer scientist, is credited with the invention of the World Wide Web.\"]\n",
    "\n",
    "# Combine questions, answers, and context\n",
    "input_sequences = [f\"question: {q} context: {c} answer: {a}\" for q, c, a in zip(questions, context, answers)]\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "tokenized_input = tokenizer(input_sequences, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "\n",
    "# Labels (target) - treat it as a text-to-text problem\n",
    "tokenized_labels = tokenizer(answers, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "\n",
    "# Model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Fine-tuning\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "# Example: Fine-tuning for a few epochs (you might need more depending on your dataset)\n",
    "model.fit(tokenized_input, tokenized_labels[\"input_ids\"], epochs=3, batch_size=2)\n",
    "\n",
    "# Save the fine-tuned model for later use\n",
    "model.save_pretrained(\"fine_tuned_t5_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
