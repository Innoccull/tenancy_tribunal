{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Processed Tribunal Order\n",
    "\n",
    "Load, process and extract feeatures from tribunal orders. To do this we will compute the below for each document:\n",
    "- Key terms (via TF-IDF)\n",
    "- Named entities (via spaCy)\n",
    "- Classify documents (spaCy, need training data for this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "\n",
    "tribunal_orders = pd.read_csv('data\\\\tribunal_orders.csv')\n",
    "\n",
    "tribunal_orders_tfidf = list(tribunal_orders['tfidf'])\n",
    "tribunal_orders_ner = list(tribunal_orders['ner'])\n",
    "tribunal_orders_cat = list(tribunal_orders['cat'])\n",
    "tribunal_orders_sum = list(tribunal_orders['sum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "documents = tribunal_orders_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "stop_words.append('tenant')\n",
    "stop_words.append('landlord')\n",
    "stop_words.append('tenancy')\n",
    "stop_words.append('suppressed')\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Save the TF-IDF matrix to a file using pickle\n",
    "with open(\"models\\\\tfidf\\\\tfidf_matrix.pkl\", \"wb\") as matrix_file:\n",
    "    pickle.dump(tfidf_matrix, matrix_file)\n",
    "\n",
    "with open(\"models\\\\tfidf\\\\tfidf_vectorizer.pkl\", \"wb\") as matrix_file:\n",
    "    pickle.dump(tfidf_vectorizer, matrix_file)\n",
    "\n",
    "# Get feature names (terms)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a dictionary to store key terms for each document\n",
    "key_terms_per_document = defaultdict(list)\n",
    "\n",
    "# Extract top N key terms per document\n",
    "num_key_terms = 5  # Number of key terms to extract per document\n",
    "\n",
    "for i, document in enumerate(documents):\n",
    "    feature_index = tfidf_matrix[i, :].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
    "\n",
    "    # Sort terms by their TF-IDF score\n",
    "    sorted_items = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Extract top N key terms for each document\n",
    "    for term_index, score in sorted_items[:num_key_terms]:\n",
    "        key_terms_per_document[i].append(feature_names[term_index])\n",
    "\n",
    "# Print key terms for each document\n",
    "#for doc_index, terms in key_terms_per_document.items():\n",
    "#    print(f\"Key terms for Document {doc_index + 1}: {terms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TF-IDF key terms for each tribunal order\n",
    "tribunal_tfidf = key_terms_per_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get the documents \n",
    "documents = tribunal_orders_ner\n",
    "\n",
    "# Process each document to extract named entities\n",
    "named_entities_per_document = []\n",
    "for document in documents:\n",
    "    doc = nlp(document)\n",
    "    named_entities = [ent.text for ent in doc.ents]\n",
    "    named_entities_per_document.append(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of named entities per document to strings\n",
    "named_entities_documents = [' '.join(entities) for entities in named_entities_per_document]\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents with named entities\n",
    "X = vectorizer.fit_transform(named_entities_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vectorizer\n",
    "with open(\"models\\\\ner_bow\\\\ner_bow_model.pkl\", \"wb\") as matrix_file:\n",
    "    pickle.dump(vectorizer, matrix_file)\n",
    "\n",
    "# Save the matrix\n",
    "with open(\"models\\\\ner_bow\\\\ner_bow_matrix.pkl\", \"wb\") as matrix_file:\n",
    "    pickle.dump(X, matrix_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save named entities to processed tribunal order\n",
    "tribunal_named_entities = defaultdict(list)\n",
    "\n",
    "for idx, doc in enumerate(named_entities_per_document):\n",
    "    tribunal_named_entities[idx] = doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the documents \n",
    "documents = tribunal_orders_ner\n",
    "\n",
    "# Process each document to extract named entities\n",
    "noun_chunks_per_document = []\n",
    "for document in documents:\n",
    "    doc = nlp(document)\n",
    "    noun_chunks = [chunk.text for chunk in doc.noun_chunks]\n",
    "    noun_chunks_per_document.append(noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of named entities per document to strings\n",
    "noun_chunks_documents = [' '.join(entities) for entities in noun_chunks_per_document]\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents with named entities\n",
    "X = vectorizer.fit_transform(noun_chunks_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vectorizer\n",
    "with open(\"models\\\\nph_bow\\\\nph_bow_model.pkl\", \"wb\") as matrix_file:\n",
    "    pickle.dump(vectorizer, matrix_file)\n",
    "\n",
    "# Save the matrix\n",
    "with open(\"models\\\\nph_bow\\\\nph_bow_matrix.pkl\", \"wb\") as matrix_file:\n",
    "    pickle.dump(X, matrix_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save named entities to processed tribunal order\n",
    "tribunal_noun_chunks = defaultdict(list)\n",
    "\n",
    "for idx, doc in enumerate(noun_chunks_per_document):\n",
    "    tribunal_noun_chunks[idx] = doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the saved model\n",
    "path_to_saved_model = 'models\\\\tribunal_order_classification_model\\\\text_cat_model'\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(path_to_saved_model)\n",
    "\n",
    "# Now, you can use the loaded model for various tasks\n",
    "text = \"The landlord entered the property.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "result = []\n",
    "\n",
    "for order in tribunal_orders_cat:\n",
    "    doc = nlp(order)\n",
    "    result.append(doc)\n",
    "\n",
    "# Save named entities to processed tribunal order\n",
    "tribunal_categories = defaultdict(list)\n",
    "\n",
    "for idx, doc in enumerate(result):\n",
    "    tribunal_categories[idx] = doc.cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load pre-trained BART model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for order in tribunal_orders_sum:\n",
    "\n",
    "    # Sample text for summarization\n",
    "    input_text = order\n",
    "\n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(input_ids, max_length=500, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    result.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save named entities to processed tribunal order\n",
    "tribunal_summaries = defaultdict(list)\n",
    "\n",
    "for idx, sum in enumerate(result):\n",
    "    tribunal_summaries[idx] = sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty DataFrame with specific columns\n",
    "columns = ['text','summary', 'key_terms', 'named_entities', 'categories', 'noun_chunks']\n",
    "processed_orders = pd.DataFrame(columns={col: [] for col in columns})\n",
    "\n",
    "processed_orders['text'] = tribunal_orders['sum']\n",
    "processed_orders['summary'] = tribunal_summaries\n",
    "processed_orders['key_terms'] = tribunal_tfidf\n",
    "processed_orders['named_entities'] = tribunal_named_entities\n",
    "processed_orders['categories'] = tribunal_categories\n",
    "processed_orders['noun_chunks'] = tribunal_noun_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_orders.to_csv('processed_orders.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts for improvement:\n",
    "- For summarising remove the superfluous sentences so that they don't dominate the summary. I am most interested in the 'nature' of the case as opposed to the outcomes because I want people to be able to get tribunal orders that match their circumstances. \n",
    "- Remove numbers for sentences (doesn't help with understanding but can be used to identify the key sentences)\n",
    "- Tidy up format in which data in processed order is stored\n",
    "- Remove dates\n",
    "- Remove frequent acronyms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
